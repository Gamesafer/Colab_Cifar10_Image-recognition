{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yK6tx3k3blhW"
   },
   "source": [
    "\n",
    "# 16124278 王浩 Week 3 \n",
    "## Cifar10图像分类\n",
    "\n",
    "由于Cifar10数据集较大，且本文中的训练模型的总参数多达142万，\n",
    "\n",
    "即使在本地使用GPU（MX150）训练，每次运行仍需接6-8小时，不利于程序的后续调整，\n",
    "\n",
    "故本程序借助Google Colab（约30min-1h）利用GPU加速在云端运行。\n",
    "\n",
    "最终模型在（最佳的一次参数：batch=256,factor=0.1,patience=5,62s, 35epoch）\n",
    "\n",
    "训练集上的准确率为：99.78%\n",
    "\n",
    "验证集上的准确率为：97.15%   \n",
    "\n",
    "**测试集上的准确率为：97.07%**\n",
    "\n",
    "*在几大经典图像识别数据集（MNIST / CIFAR10 / CIFAR100 / STL-10 / SVHN / ImageNet）中，*\n",
    "\n",
    "*对于 CIFAR10 数据集而言，目前业内 State-of-Art 级别的模型所能达到的最高准确率是 96.53%。*\n",
    "\n",
    "*注：由于暂时无法在Colab中引用本地图片，本文中所有图片均已上传至GitHub，用网络链接的形式进行展示。*\n",
    "\n",
    "[本地图片链接（GitHub）](https://github.com/MirstT/Colab_Cifar10_Image-recognition/tree/master/res)\n",
    "\n",
    "[本程序所有历史训练数据](https://github.com/MirstT/Colab_Cifar10_Image-recognition.git)\n",
    "\n",
    "## 打印Colab目前连接使用的机器（GPU）信息\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "id": "DkvdQuRAavXd",
    "outputId": "aa23660b-215b-4232-b732-a31a6c828d59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gputil in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
      "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
      "Gen RAM Free: 13.0 GB  | Proc size: 114.5 MB\n",
      "GPU RAM Free: 11441MB | Used: 0MB | Util   0% | Total 11441MB\n"
     ]
    }
   ],
   "source": [
    "# 检查并安装第三官方库\n",
    "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
    "!pip install gputil\n",
    "!pip install psutil\n",
    "!pip install humanize\n",
    "import psutil\n",
    "import humanize\n",
    "import os\n",
    "import GPUtil as GPU\n",
    "GPUs = GPU.getGPUs()\n",
    "gpu = GPUs[0]\n",
    "def printm():\n",
    " process = psutil.Process(os.getpid())\n",
    " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
    " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
    "\n",
    "#打印相关信息\n",
    "printm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iO_7KHYAcASL"
   },
   "source": [
    "## 建立Ipynb与Google云盘的连接\n",
    "\n",
    "将训练所得模型以及日志文件储存在自己的云盘文件中，同时方便以后使用云盘上的本地数据集/本地模板库。\n",
    "\n",
    "![alt text](https://github.com/MirstT/Colab_Cifar10_Image-recognition/blob/master/res/WH_2019-04-09_08-08-39.png?raw=true)\n",
    "\n",
    "**登录代码：**（一次性）\n",
    "\n",
    "![alt text](https://github.com/MirstT/Colab_Cifar10_Image-recognition/blob/master/res/WH_2019-04-09_16-14-21.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "HznycZWvcIBs",
    "outputId": "c8a29d7d-8a16-4ae9-910a-97ad634f432e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: Package 'python-software-properties' has no installation candidate\n",
      "Selecting previously unselected package google-drive-ocamlfuse.\n",
      "(Reading database ... 131304 files and directories currently installed.)\n",
      "Preparing to unpack .../google-drive-ocamlfuse_0.7.3-0ubuntu1~ubuntu18.04.1_amd64.deb ...\n",
      "Unpacking google-drive-ocamlfuse (0.7.3-0ubuntu1~ubuntu18.04.1) ...\n",
      "Setting up google-drive-ocamlfuse (0.7.3-0ubuntu1~ubuntu18.04.1) ...\n",
      "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
      "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
      "··········\n",
      "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
      "Please enter the verification code: Access token retrieved correctly.\n"
     ]
    }
   ],
   "source": [
    "# 安装相关文件\n",
    "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
    "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
    "!apt-get update -qq 2>&1 > /dev/null\n",
    "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
    "\n",
    "# 账号信息授权\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "#授权码1\n",
    "#4/JwE_0WWiynLrN7mj3bfRDFe6R4jhjc2hKcSb59vXE816ZAyt2kCyjXM\n",
    "\n",
    "# 账号密码授权\n",
    "from oauth2client.client import GoogleCredentials\n",
    "creds = GoogleCredentials.get_application_default()\n",
    "import getpass\n",
    "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
    "vcode = getpass.getpass()\n",
    "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
    "#授权码2\n",
    "#4/JwHPn1brf-kYZU5L6pmu4XsF7Ckdhs-h9aXh93BLCYk-bMQKa1r-dks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BRNOLbiwqPrj"
   },
   "source": [
    "## Google云盘工作文件夹设置\n",
    "显示工作目录下的内容（和linux系统下命令基本相同）\n",
    "\n",
    "ubuntu18.04.1\n",
    "\n",
    "![alt text](https://github.com/MirstT/Colab_Cifar10_Image-recognition/blob/master/res/WH_2019-04-09_15-36-19.png?raw=true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "6Fz3r1dogQIn",
    "outputId": "4cc29997-a8d1-45ae-8a57-211f34f5da44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advanced_Data_Analysis\tmnist_mlp.ipynb\t\t     WH_2019-04-09_08-08-39.png\n",
      "char-09.ipynb\t\tmodels\n",
      "logs\t\t\told_Cifar_10_图像识别.ipynb\n"
     ]
    }
   ],
   "source": [
    "# 指定Google Drive云端硬盘的根目录，名为wh_drive\n",
    "!mkdir -p wh_drive\n",
    "!google-drive-ocamlfuse wh_drive\n",
    "\n",
    "\n",
    "# 指定当前的工作文件夹\n",
    "import os\n",
    "os.chdir(\"wh_drive/Colab\") \n",
    "\n",
    "\n",
    "# 显示工作目录下的内容\n",
    "! ls "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wVUjk4lb7Lj-"
   },
   "source": [
    "## Cifar10数据集介绍\n",
    "该数据集共有60000张彩色图像，这些图像是32*32，分为10个类，每类6000张图。这里面有50000张用于训练，构成了5个训练批，每一批10000张图；另外10000用于测试，单独构成一批。测试批的数据里，取自10类中的每一类，每一类随机取1000张。抽剩下的就随机排列组成了训练批。注意一个训练批中的各类图像并不一定数量相同，总的来看训练批，每一类都有5000张图。\n",
    "  \n",
    "下面这幅图就是列举了10各类，每一类展示了随机的10张图片：\n",
    "  \n",
    "  ![alt text](https://github.com/MirstT/Colab_Cifar10_Image-recognition/blob/master/res/Cifar10%E6%95%B0%E6%8D%AE%E9%9B%86.png?raw=true)\n",
    "  \n",
    "## Cifar10数据集下载\n",
    "服务器网速很快(6-10MB/s)，不需要从云盘中读取数据集，直接下载到Colab服务器运行即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "9UFA0AEIhDJM",
    "outputId": "afb951ed-a14f-4f60-e429-5222c9c5708d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 26s 0us/step\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#设置随机种子\n",
    "np.random.seed(161)\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "#读取数据集\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "#归一化\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CqTpjQP97xiz"
   },
   "source": [
    "## Cifar10数据预处理\n",
    "将彩色图片转为灰度图片：\n",
    "灰度值 = 0.2989 * 红色 + 0.5870 * 绿色 + 0.1140 * 蓝色\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "colab_type": "code",
    "id": "1NQbPEnNhpTD",
    "outputId": "350f73c8-41e8-45cf-d9d4-7f2a2b0b92e2"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJztnWuQnGeV3/+nb3O/j2Y0kkYaSZaE\nbPmKUGzsGLIEbFi2DLUbF3wg/kCtt1JQCZXNBxdbFUhVPrCpAMWHhJQJrjUbgiELLC7DZvEaL4Y1\ntpFvsmTZsqy7NDO6jnoufe+TD92ukuXn/8zIsnrsvP9flUo9z+mn39Pv+5737X7+fc4xd4cQInmk\nltsBIcTyoOAXIqEo+IVIKAp+IRKKgl+IhKLgFyKhKPiFSCgKfiESioJfiISSuZzJZnYngG8BSAP4\nn+7+tdjze3r7fGhkNGgrFxfovGq5GBx3Nzonm2untlwbt6WzOWpLpcLbKxbm6JxyqUBtXqtRm4G/\nt1Q6zeelwtfzru4eOqctsj+8VqW2QoEfMyD8y9G61+mMYoHvq1rEj9ivVJmpWuV+1Oux1+PzMhke\nTpkMP2aO8HkQ+/FtnbhRWCigVCrzk+dCn5bypBBmlgbw3wB8FMAxAL83s4fd/WU2Z2hkFH/xjf8e\ntB175Vm6rVMH9wbHazXu/uja91Hb2o1bqW1g5Vpqa+8Ib2/fnifpnMP7d1FbZZZfNNKR99Y70Edt\nmfbO4PiOW2+nc67azPdV8fxZatuz+3lqq9fLwfFyJXwhB4CX97xEbfmZ09RWKpeorVIOB93ZM/zC\nNbfAfazW+LZWrBiktoHBbmqr+Wx4WxU6BcVC+Mrwj48/xSddxOV87N8BYL+7H3D3MoCHANx1Ga8n\nhGghlxP8qwEcveDvY80xIcR7gCu+4Gdm95rZTjPbOZs/f6U3J4RYIpcT/McBjF/w95rm2Jtw9/vd\nfbu7b+/p5d9VhRCt5XKC//cANpnZejPLAfgMgIffGbeEEFeat73a7+5VM/sigL9HQ+p7wN33xObU\najXkz4VXj4f6+UqprwjLg57ppXPG1m7gftT5MmqqzleB6wthual47gyd4wW+crx6eITa1o5fRW3j\nV62jtlWr1wTHR4jECgDZbBu1VfvD6gEAjK9ZyedVw6v9xSKX82bOcfXj9GmuOmQisi4svNo/MMTf\nc3sX9/F8/hy1tbXzcKo7lyqzmbAv+fMzdE65FF7td6YBBrgsnd/dfwHgF5fzGkKI5UG/8BMioSj4\nhUgoCn4hEoqCX4iEouAXIqFc1mr/JeMOVMIyW7nE5beFhbBsNLGZ/5p4bn6e2mLJJYPDkaSZbPha\nuWnTZjrngzdvp7bVo2FZDgD6+lZQWyXDswE728OyUSaSIWbVSObePJffSuRYAkBnR1giHOjn8ubG\nDVdT2969r1IbjPtRKoWl277eATonktiJ8/lpanOEz1Mgnil47lz4XC0s8CQilvF3KX04dOcXIqEo\n+IVIKAp+IRKKgl+IhKLgFyKhtHS13+t1VElih1X5CnZbriM4fv40L+00tJKvpK+9hifNjIyvorYs\nWwaO1FuqVLmy8MokTwhaOHCKv2aKryq/+tKLwfEPbOUr6bfv+AC1xVaP85H6DEcOnwiO57KR2oo5\nnqg1vIIrO0eOvsZfk5Q1mytwNSif5+dVJsvL4/X28iSoWL1DVp4wVmewrS18LtqSqvc10J1fiISi\n4BcioSj4hUgoCn4hEoqCX4iEouAXIqG0XOorLYQllu4OLgH1DoaTXG66/gY6Z3zDJmqbjSSyvHrg\nKLXlF8JyzdwMr7V2ZobLeZNTvB5cbySxByme8PHID38cHM/eza/zH7rlNmrLZrmMuXIll0XhYbls\n5ly4Ow0APPc8726UidQZ7OrhEmG1FpYqy3P8mKUjt8RYV55ajUuwZ85y+TCFsEQYa//V3x9OQEtH\n2oK9dbtCiESi4BcioSj4hUgoCn4hEoqCX4iEouAXIqFcltRnZocAzAKoAai6Oy9YB8BShra2bNBW\nSffQeYWO7uD4wTxvq/TCb5+htrNneF264yd4jbZsOpwylU3x7KsSaVsFAMUit42t4Ifm5NRhausl\n2V6zM3k6Z9/Bg9yPsWFqy2a5j2Pj4VZeq8g4AByZ4jLrqy9x28gYl0UPHSESW4Ufs3qZ22qR+ont\nOS5HtmXC5z0AFIrh1+zt5RJmhrT4sku4n78TOv+/cCeirhDiXYs+9guRUC43+B3AL83sWTO7951w\nSAjRGi73Y/9t7n7czEYAPGpmr7j7Exc+oXlRuBcA+gf4TyOFEK3lsu787n68+f9JAD8FsCPwnPvd\nfbu7b+/qDi/cCSFaz9sOfjPrMrOeNx4D+BiA3e+UY0KIK8vlfOwfBfBTa1QMzAD43+7+f2MTUqkM\nOjtHg7aTMzzTbv/RsMzz8h5+rUlFZKhapDVYYZYXdkwTSa9Q4jLazCy3zUZaYR06tpfaujq4LLpl\n45awISI5/tNv/pHa1q1fT22bt/A2ZUND4ayztnZ+XPp6uVSWqvJiofMlfg9jLa8KMzy7sFbjRVfb\nO7hkN5fnr9kbyTxsaw9n4pXLsRZ24QzTep3LlBfztoPf3Q8AuP7tzhdCLC+S+oRIKAp+IRKKgl+I\nhKLgFyKhKPiFSCgtLeCZTmfQPxjOEtt/dB+dN3konHXWmeWFLM/P8+KYc/mT1GYRqWRmNizNzRS4\nNJQhWYwAMDw6Qm0dPWGpDABWT3CRZZzIRgdf/B2dkzYuA1ZqPIvt1GlenPTaa7cGx6/atIHOGY9k\n53XffCO17XrlCLWViuHCsKVsJKsPXJarO5ekp6bC/QkBINfGZcy+AXYecNm5UAhntNZ96VKf7vxC\nJBQFvxAJRcEvREJR8AuRUBT8QiSUlq72l0rzeP31cG29V17fT+edmHw9OF6LJOH09HVR25ZNE9S2\nbes2aps8FV5hPXyK+7FiZTiRCQDWbeRJMz1DXAmYPse356fDysiRw3xF/FSkpdjWq6kJH90cXtEH\ngPk5shrNxQN4masOe57iasWmLbxt2+jq/uD4U888ERwHgKlpnoxVqfDV/mKB+38u0qasozvsY2zl\nfp60vbuUxB7d+YVIKAp+IRKKgl+IhKLgFyKhKPiFSCgKfiESSkulvvm5PJ564tGwI6Ok9hyAjVuv\nDY53RNoqbb16E7Vt2byG2mrFcGIMAHgqLF/NgzcsymTDiSUAkE6HJR4AqFR5Isj87Flq6yuHpahq\nzemcIyd5ElR793G+rd4BatuwcSI47pH7TWEmXJcOAF55+gVq8wI/D7bdcWdw/NrreIJRYSeX+l7f\nf4jaOjt5deq+/iFqa3S7eyv5PD8upVJ4X7mkPiHEYij4hUgoCn4hEoqCX4iEouAXIqEo+IVIKItK\nfWb2AIBPAjjp7tuaY4MAfghgAsAhAHe7O9clmlTKVZw8GpbFbrz+D+m8trZwbbdBrsphbBWvw3Y2\n0qrp6H4uo5XrYfktZTxVLZ3h0kvNeQ1CVGPtxsKSIwB4Lby97r5w7UQAODPHswRTOZ4dWXcuHza6\nt4cm8Rnd7fyYTawap7b2NPcjhXDdxWu38YzK/n4uwT5c+CW1TU3yEFg9soraahauAZmNtJzL58Ny\n5N5suLVdiKXc+f8KwMVi6X0AHnP3TQAea/4thHgPsWjwu/sTAC6+Hd4F4MHm4wcBfOod9ksIcYV5\nu9/5R919svl4Co2OvUKI9xCX/fNed3czo1+6zOxeAPcCQDbLa9gLIVrL273zT5vZGAA0/6ddMNz9\nfnff7u7bM5mWphIIISK83eB/GMA9zcf3APjZO+OOEKJVLEXq+wGADwMYNrNjAL4C4GsAfmRmnwdw\nGMDdS9lYKpVBZ/dg0JaNqEYzM+EPFm2DXJJZqHJNqci7a6FjoIfa2upGXpBLfR7Zw8UKz2Jr7+AT\nU5H2WvVUeF73EJeacs7lzXQHz9zzHNda6xZ+b1bj0mEqzd9ztitHbR3d3FYthWXdM8en6ZyhLt42\n7K5P3EFtO188RG1zkeKexdKp4HiJtOQCgP6e8LmfSUf074ufu9gT3P2zxPSRJW9FCPGuQ7/wEyKh\nKPiFSCgKfiESioJfiISi4BciobT0Vze5XBvG1oazqSzFr0PFYjiDaTrP3c/18yy2SpVLQxb5FWJh\nLpwhVnHueybDC3FW09zW2csz3EaGZqjNz4bloXKkx5zVuf8dHR3UloqoSnUPb69W47JoKhspnprm\nPs7N8yxNIwUt2yLnW/4UlwE7OsNSNQDcfst11Pbq64epbffLU8HxuTzPtsyRwrD1eizT8s3ozi9E\nQlHwC5FQFPxCJBQFvxAJRcEvREJR8AuRUFoq9bkBbmE5pxKRohZmw1JOW0SGms1HCnEWeeHMhTyX\njbIkqa+ni0t2Kwa4NNQ7yDPcVvTz91bL9FFboS28H8+u41l9pdoktSGSeVirRrILSQZkLcWzLS0i\n9fUP8uzCei3iIzmv+vr4/s3x2jSYmY3IrJWwFAwAN2xdSW39PeHz55FHeLHQU9PhQrjVSBxdjO78\nQiQUBb8QCUXBL0RCUfALkVAU/EIklNaW03UHyApxps5XjvvCOQwY7yPL7wDet4HX9+tu5yu9aePX\nw/l8eKW3uHCezunoqlDblk1cCRhft4baUtl11DY3E/ZxfGyM+3GQFl9G7yDZ+QAGB3jyUSYTTp6K\n5Z14JFGovauT2qpFvsKdItvLxhLJwNWgoeFuaptb4KrD/Ew4eQcAVq8I1wz81B99jM7525//Q3A8\nk1l6DT/d+YVIKAp+IRKKgl+IhKLgFyKhKPiFSCgKfiESylLadT0A4JMATrr7tubYVwH8KYA3+gx9\n2d1/sdhr9XR14kO3vD9o23D19XTeiePHg+OrV3GpbPOmjdS2csUItaWdy4ezJKmjFEl+sRR/ve4u\nntjT3c0ltnSOS5VZIpkW5sMtoQDgpm1cOpzYPEFtlTqXMZ3cV6p1Lst5mu+rdJafqpUi1w/rJNEl\nleH3PWvnfiAyr1Th+yOT5rUha+XwebUiIive9s8/EBz/3TMv0TkXs5Q7/18BuDMw/k13v6H5b9HA\nF0K8u1g0+N39CQA8P1YI8Z7kcr7zf9HMdpnZA2bGk62FEO9K3m7wfxvARgA3AJgE8HX2RDO718x2\nmtnOuXle7EAI0VreVvC7+7S719y9DuA7AHZEnnu/u2939+3dXXwBQwjRWt5W8JvZhVkinwaw+51x\nRwjRKpYi9f0AwIcBDJvZMQBfAfBhM7sBgAM4BODPlrKxzs4OvP+69wVt19zIpb7CtrBs19XHs8p4\npTjAjUs5qYgkM9gVrsMW6dYVvbrWSSspYJFabBFJqVQKt+vaeNVaOqcjxyXHwjzPWPRU5PSxsM0j\n9fHqzm21yDGLtagqF8L7o1bn7zmViZwfkSM6e4ZLvocPHqW2W2+7MTi+UOH1JDuJHBlRlt/CosHv\n7p8NDH936ZsQQrwb0S/8hEgoCn4hEoqCX4iEouAXIqEo+IVIKC0t4JlKpdBBMtm623nLq65O4mak\nWGGsUKTFpL6YpORhaa5e4ZJdTL6ySBHJakSsjMk5TgqQdvfzDMhqjW+rVo8UhCQtuQDAUQuOp2LO\n17itluESrCNysEnBWKuH/QOAtsh7ztb4Mesq8nk+HZYcAeDUgeng+JotvIjr6VT417KXIvXpzi9E\nQlHwC5FQFPxCJBQFvxAJRcEvREJR8AuRUFoq9aXTafT0hSUnj2TTLZTCco2XeE+1EpkDAPNz89RW\nrvB5pVI4m65a5VJZJZKBV4lsayHS921hnmd7VUmmYM9gH53T08f7Gvb3DFNbey7cjw8Aaqz3okX6\n6oHbenp4QdMzJ/l+LBbCkli9zotPGfj7qtf4Odfbw+XqdWtHqa2wED4fPVLstK8nLJmnI/LxxejO\nL0RCUfALkVAU/EIkFAW/EAlFwS9EQmnpav/MTB5/+/DfBW217G/ovHPnwokPc+dP0zmpSK5HTAmY\nng5vCwBqJFtoMNL+a2B4iNra0nz3z58Nt3ACgH2v7aW2/Fx4dXt8PW/Jlc5ypaW3h/u/fj2vC7hm\nPFzvcP2G1XTOYBvPSulp5z7WI7UckQ4n21RqfCU9HWnJlY74ODoRUUZ6uRJQ8XCSUZqLDhgcDL/n\nTCTZ7WJ05xcioSj4hUgoCn4hEoqCX4iEouAXIqEo+IVIKEtp1zUO4HsARtFoz3W/u3/LzAYB/BDA\nBBotu+5293Ox18rPzuHRx58M2vrXbKHzvBaWr55/8nE6Z90aXv9seIjLV8ePTVFbldR96xzkiTHl\nFE/6mT7GWzh9ZMct1HbDdddQ20KpGBxPZfmhPnjkMLXte+11antp9/PU1t8Xbsr6x3/yaTrn1ms2\nU1su0hNtzdg4tZWJ1GeRYnexuosVUpsQAFKZSF3Afp6Y1EGSceppLkkz4TNSgvItLOXOXwXw5+5+\nNYCbAXzBzK4GcB+Ax9x9E4DHmn8LId4jLBr87j7p7s81H88C2AtgNYC7ADzYfNqDAD51pZwUQrzz\nXNJ3fjObAHAjgKcBjLr7ZNM0hcbXAiHEe4QlB7+ZdQP4MYAvuXv+Qpu7OxAunm5m95rZTjPbWS7z\nQghCiNaypOA3sywagf99d/9Jc3jazMaa9jEAJ0Nz3f1+d9/u7ttzOf77ZiFEa1k0+K3R3ua7APa6\n+zcuMD0M4J7m43sA/Oydd08IcaVYSlbfrQA+B+AlM3uhOfZlAF8D8CMz+zyAwwDuXuyFBgaH8K8+\n+6+DtraRTXTewmxYfnvtpRfpnLGVXP5JReqcdbTzDLFyPdxyafM27vvAGM/4WxjmdeQ++fF/SW2d\nPR3UNk+kvkhnLVRJGzIAKFbDrwcAJ0+epbbDB08Exzs7+f6dOnaG2g7teY3aUkXu44Gp4AdS7PjY\ndjpn3cQqaotlA6baI2l4WS4DGqvVZ3xOzsLH7FKkvkWD391/C4C95EeWvikhxLsJ/cJPiISi4Bci\noSj4hUgoCn4hEoqCX4iE0tICnmZAWy58vdn3ym46L38+LPV5LPuqzDOi5iLtuiyilbS3hXOpKgu8\nfdb5U9zH6SM8q+/v/j5c6BQAzs1Gtjd3Pjje08sltr6BcAs1AOiKFJ48diws5wHAyHC4UGd7L5c+\nf/Nz/p7PvraL2mpl3hJt/1S4IOuxSMuzTVu5dNvX28ltA7wlWkcnz+rr6wqfV9l2XoyzszN8XNyX\nrvXpzi9EQlHwC5FQFPxCJBQFvxAJRcEvREJR8AuRUFoq9dWrFcyeCct2v/rZz+m8o1PHguOpSjjL\nDgB27cpTWyz1qVrlWVsgmVSPPvIrOiWX5VLZDTfeRG3lXA+15UsL1HbgSDiL7cwZ3t+vXORZfSem\nDlHbwUP8Nbff+P7g+L/9wr+nc5556nfUVj3PM/7yJV4kphCuMYMDO7nM+ptnJ6mtK8NlxWyOS3Pp\nNn4e9BCpb826CTrnrj/+THC8XF36/Vx3fiESioJfiISi4BcioSj4hUgoCn4hEkpLV/uz2RzGRseC\ntk0T6+k8R3g1OhNphZWOrOin0vya53WeiJNr7wobsjxpY9WqcIILAHz4jjuoraczkkDSzmv/vbw7\nXNdw337edmvl6glqK0baZKU7uI+7970SHH953z46p3NiK7WdOMHf80A/t43kwnX1Ort5HcSzU7x9\n2Znj+6nt1OlwEhEAFGuRJDRSYHFyhofnBz8SnlPlZf/egu78QiQUBb8QCUXBL0RCUfALkVAU/EIk\nFAW/EAllUanPzMYBfA+NFtwO4H53/5aZfRXAnwI41Xzql939F7HXqlarOHsq3OLp5n/2QTrvgx/6\nUHC8rY0nUmQicl6sXVc90roqjfD2KmWurxTKPAnnzLGD1Ha2yBNIzp7mbbIOEEnvxMlwQhUAdI/w\n9lRo4zKm5bjUV66Gk20e/fVv6Zx1G6+ltvFBLpm2p/hp3EkSq0pFXsPvQH4PtXX38FqINedJYVPn\n5qhteHgiOL5Q4efir379THB8dpbXp7yYpej8VQB/7u7PmVkPgGfN7NGm7Zvu/l+XvDUhxLuGpfTq\nmwQw2Xw8a2Z7AfDLsBDiPcElfec3swkANwJ4ujn0RTPbZWYPmBn/mZUQ4l3HkoPfzLoB/BjAl9w9\nD+DbADYCuAGNTwZfJ/PuNbOdZrZzdo5/zxJCtJYlBb+ZZdEI/O+7+08AwN2n3b3m7nUA3wGwIzTX\n3e939+3uvr2nm1enEUK0lkWD3xotbL4LYK+7f+OC8QszdD4NgLfcEUK861jKav+tAD4H4CUze6E5\n9mUAnzWzG9CQ/w4B+LPFXiiVMnSRNkNn8kU67/ldzwbHR0b4MsPoyDC1VSpcRjt3bobaUAz7mKnz\n11u9nsto4wP8k9DxfbyO3Pwcr1k3MroyON451E/npNu5fLVQ4MdlbGwttU2dCNddPH0m3E4MAMZW\nRdqoRVqzzZX4/kcmfL5V6lyebesg2ZsA2iLZouUzp6gNqXCdPgAYJVmV5RJvOcd2B99Lb2Upq/2/\nBRB6x1FNXwjx7ka/8BMioSj4hUgoCn4hEoqCX4iEouAXIqG0tIBnyoC2bDhTqVTkEtuTTz4WHPcK\nl6F6O3mBxkqFZ18VC7wFWIZcK9dNjNM5226+mto2ruUy4MzRsFQGAFPnTlNbriMsbW0cCkuAAHDq\nFM84u3bLNmq75tot1PbQ//pecDyDcEFNAKjM8+NZLnObx6pWtoePdax91sT6DdR28uirfFspnmXa\n0cW3t3Xr5uB4cYEfl/GxkeD4r3NcUrwY3fmFSCgKfiESioJfiISi4BcioSj4hUgoCn4hEkpLpb56\nvY6FAiloGSmqecfHPxl+vTLPAktH5Lx6jRdG9DSXa9KZsEzV3sULWU7NcOlwdob3rTtb4P5bOy+q\n+eoLB4LjZ37HM842rOeS3Qeu2kRt5UjGX0cuLG15JKMylkGYSvNTlbS6AwAU6qTPY43v33VruNRX\nnDtDbVf38mzAZ559ntpOHA7Lh4V5fn77wrngeLnEMz4vRnd+IRKKgl+IhKLgFyKhKPiFSCgKfiES\nioJfiITS2qy+lKGrOyyX9UUqD/asCGc9lSKyRnvkupYznlnmHTwbsK0zPK9e5NlXs7N5akt38sKZ\nIxt5wc2NnTyr77WD4V59MC5hZklRVQA4PnmE2oaGeQFVZisXuHxVKvHinvORjL9SJPutUgpLy5l2\nLs+OrlpBbYcnp6lt+gjZ9wCKc/y9vb7nheD40BD3wwcGw+ORQqcXozu/EAlFwS9EQlHwC5FQFPxC\nJBQFvxAJZdHVfjNrB/AEgLbm8//G3b9iZusBPARgCMCzAD7n7ry/EIB6vYiFWZLMUufXoax1B8en\np/kK6msvH6K29gxf0c/18VX2YdIebNVwH52TiSQsDfUNUVsk9wjFQjipAwBGRsIKwupV4dVhAJic\nmqK2ffv2UttEeT21MSVmdpYfs4UFvpKeP89Vk9hqf60cTqxKt/EknD27eau3WAutkZFRalt9Ha+F\nOLIiPG94Ba+72E78f+yfHqdzLmYpd/4SgD9w9+vRaMd9p5ndDOAvAXzT3a8CcA7A55e8VSHEsrNo\n8HuDNy6t2eY/B/AHAP6mOf4ggE9dEQ+FEFeEJX3nN7N0s0PvSQCPAngdwIy7v5EUfQzA6ivjohDi\nSrCk4Hf3mrvfAGANgB0A3rfUDZjZvWa208x2zs6SQh5CiJZzSav97j4D4HEAtwDoN7M3FgzXADhO\n5tzv7tvdfXtPD/9JpRCitSwa/Ga2wsz6m487AHwUwF40LgJ/0nzaPQB+dqWcFEK88ywlsWcMwINm\nlkbjYvEjd3/EzF4G8JCZ/WcAzwP47qKvVHfUSdulVOQ6lKmEk1J6SesvAHj2qV9T29Q0T4yxLE9y\n2bHj/cHx227ZTuecP8+lrV3PPU1t80WeyLLvyFFqO3DoUHC8sMC/crnzInjtvTy5JJ+fpbZZ0lJs\nPs9lykgpPmTS3NoX+US5an1YjhwYGqNzRlZxiW3VjddS22Ckhl8uVhuS2SLJWPBwvKQiLcMuZtHg\nd/ddAG4MjB9A4/u/EOI9iH7hJ0RCUfALkVAU/EIkFAW/EAlFwS9EQrFLqfl12RszOwXgcPPPYQBc\nc2sd8uPNyI83817zY527c332Aloa/G/asNlOd+cCufyQH/Ljivqhj/1CJBQFvxAJZTmD//5l3PaF\nyI83Iz/ezP+3fizbd34hxPKij/1CJJRlCX4zu9PMXjWz/WZ233L40PTjkJm9ZGYvmNnOFm73ATM7\naWa7LxgbNLNHzey15v+8F9aV9eOrZna8uU9eMLNPtMCPcTN73MxeNrM9ZvbvmuMt3ScRP1q6T8ys\n3cyeMbMXm378p+b4ejN7uhk3PzSL9J1bCu7e0n8A0miUAdsAIAfgRQBXt9qPpi+HAAwvw3ZvB3AT\ngN0XjP0XAPc1H98H4C+XyY+vAvgPLd4fYwBuaj7uAbAPwNWt3icRP1q6T9DIbu5uPs4CeBrAzQB+\nBOAzzfH/AeDfXM52luPOvwPAfnc/4I1S3w8BuGsZ/Fg23P0JAGcvGr4LjUKoQIsKohI/Wo67T7r7\nc83Hs2gUi1mNFu+TiB8txRtc8aK5yxH8qwFcWI1iOYt/OoBfmtmzZnbvMvnwBqPuPtl8PAWAF4G/\n8nzRzHY1vxZc8a8fF2JmE2jUj3gay7hPLvIDaPE+aUXR3KQv+N3m7jcB+DiAL5jZ7cvtENC48qNx\nYVoOvg1gIxo9GiYBfL1VGzazbgA/BvAld39Tl45W7pOAHy3fJ34ZRXOXynIE/3EA4xf8TYt/Xmnc\n/Xjz/5MAforlrUw0bWZjAND8/+RyOOHu080Trw7gO2jRPjGzLBoB9313/0lzuOX7JOTHcu2T5rYv\nuWjuUlmO4P89gE3NlcscgM8AeLjVTphZl5n1vPEYwMcA7I7PuqI8jEYhVGAZC6K+EWxNPo0W7BMz\nMzRqQO51929cYGrpPmF+tHqftKxobqtWMC9azfwEGiuprwP4i2XyYQMaSsOLAPa00g8AP0Dj42MF\nje9un0ej5+FjAF4D8A8ABpfJj78G8BKAXWgE31gL/LgNjY/0uwC80Pz3iVbvk4gfLd0nAK5Doyju\nLjQuNP/xgnP2GQD7AfwfAG0GXuFQAAAANklEQVSXsx39wk+IhJL0BT8hEouCX4iEouAXIqEo+IVI\nKAp+IRKKgl+IhKLgFyKhKPiFSCj/D/EcoRMKOMsFAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFAW/EImi4BciURT8QiSKgl+IRPlf7Ia35BeFvPMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#选取彩色通道，将图片转换为灰度图\n",
    "x_train_gray = np.dot(x_train[:,:,:,:3], [0.299, 0.587, 0.114])\n",
    "x_test_gray = np.dot(x_test[:,:,:,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "#大小统一为32*32像素\n",
    "x_train_gray = x_train_gray.reshape(-1,32,32,1)\n",
    "x_test_gray = x_test_gray.reshape(-1,32,32,1)\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "y_train_cat = to_categorical(y_train)\n",
    "y_test_cat = to_categorical(y_test)\n",
    "\n",
    "plt.imshow(x_train[1])\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(x_train_gray[1,:,:,0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GrdTStoIeQw3"
   },
   "source": [
    "## 模型搭建（基于Inception网络）\n",
    "\n",
    "### Inception网络是对传统CNN网络的改进\n",
    "在 Inception 出现之前，大部分流行 CNN 仅仅是把卷积层堆叠得越来越多，使网络越来越深，以此希望能够得到更好的性能。\n",
    "\n",
    "**但问题是：图像中突出部分的大小差别很大。**\n",
    "\n",
    "例如，狗的图像可以是以下任意情况。每张图像中狗所占区域都是不同的。\n",
    "\n",
    "![alt text](https://github.com/MirstT/Colab_Cifar10_Image-recognition/blob/master/res/%E4%B8%8D%E5%90%8C%E5%8D%B7%E7%A7%AF%E6%A0%B8%E7%9A%84%E5%BF%85%E8%A6%81%E6%80%A7.jpg?raw=true)\n",
    "\n",
    "从左到右：狗占据图像的区域依次减小（[图源](https://unsplash.com/)）。\n",
    "\n",
    "* 由于信息位置的巨大差异，为卷积操作选择合适的卷积核大小就比较困难。\n",
    "\n",
    "* 信息分布更全局性的图像偏好较大的卷积核，信息分布比较局部的图像偏好较小的卷积核。\n",
    "\n",
    "* 非常深的网络更容易过拟合。将梯度更新传输到整个网络是很困难的。\n",
    "\n",
    "* 简单地堆叠较大的卷积层非常消耗计算资源。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**获得高质量模型最保险的做法就是增加模型的深度（层数）或者是其宽度（层核或者神经元数），**\n",
    "\n",
    "但是一般设计思路的情况下会出现如下的缺陷：\n",
    "\n",
    "* 参数太多，若训练数据集有限，容易过拟合；\n",
    "\n",
    "* 网络越大计算复杂度越大，难以应用；\n",
    "\n",
    "* 网络越深，梯度越往后穿越容易消失，难以优化模型。\n",
    "\n",
    "\n",
    "解决上述两个缺点的根本方法是将全连接甚至一般的卷积都转化为稀疏连接。为了打破网络对称性和提高\n",
    "\n",
    "学习能力，传统的网络都使用了随机稀疏连接。但是，计算机软硬件对非均匀稀疏数据的计算效率很差，\n",
    "\n",
    "所以在本模型中重新**启用了全连接层，目的是为了更好地优化并行运算**。\n",
    "\n",
    "### Inception架构的主要思想是找出如何用密集成分来近似最优的局部稀疏结。\n",
    "![alt text](https://github.com/MirstT/Colab_Cifar10_Image-recognition/blob/master/res/Inception%E6%9E%B6%E6%9E%84.jpg?raw=true)\n",
    "\n",
    "* 采用不同大小的卷积核意味着不同大小的感受野，最后拼接意味着不同尺度特征的融合；\n",
    "* 之所以卷积核大小采用1x1、3x3和5x5，主要是为了方便对齐。设定卷积步长stride=1之后，只要分别设定padding =0、1、2，采用same卷积可以得到相同维度的特征，然后这些特征直接拼接在一起； \n",
    "* 很多论文都表明pooling挺有效，所以Inception里面也嵌入了pooling。\n",
    "* 在 3x3 和 5x5 卷积层之前添加额外的 1x1 卷积层，实现跨通道的交互和信息整合并限制输入信道的数量（降维）减少计算成本。\n",
    "\n",
    "所有子层的输出最后会被级联起来，并传送至下一个 Inception 模块。\n",
    "\n",
    "**Inception的作用：代替人工确定卷积层中的过滤器类型或者确定是否需要创建卷积层和池化层，**即：不需要人为的决定使用哪个过滤器，是否需要池化层等，由网络自行决定这些参数，可以给网络添加所有可能值，将输出连接起来，网络自己学习它需要什么样的参数。\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2681
    },
    "colab_type": "code",
    "id": "D7ejpLd1hz1F",
    "outputId": "8fc32070-8abb-48d6-8d79-3206ba1e3a76"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:38: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (1, 1), activation=\"relu\", name=\"inception_1_/1x1\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:41: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(96, (1, 1), activation=\"relu\", name=\"inception_1_/3x3_reduce\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:44: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), activation=\"relu\", name=\"inception_1_/3x3\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (1, 1), activation=\"relu\", name=\"inception_1_/5x5_reduce\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:50: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (5, 5), activation=\"relu\", name=\"inception_1_/5x5\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:53: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(pool_size=(3, 3), strides=(1, 1), name=\"inception_1_/pool\", padding=\"same\")`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:56: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (1, 1), activation=\"relu\", name=\"inception_1_/pool_proj\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:38: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), activation=\"relu\", name=\"inception_2_/1x1\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:41: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), activation=\"relu\", name=\"inception_2_/3x3_reduce\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:44: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(192, (3, 3), activation=\"relu\", name=\"inception_2_/3x3\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (1, 1), activation=\"relu\", name=\"inception_2_/5x5_reduce\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:50: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(96, (5, 5), activation=\"relu\", name=\"inception_2_/5x5\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:53: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(pool_size=(3, 3), strides=(1, 1), name=\"inception_2_/pool\", padding=\"same\")`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:56: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (1, 1), activation=\"relu\", name=\"inception_2_/pool_proj\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:38: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(192, (1, 1), activation=\"relu\", name=\"inception_3_/1x1\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:41: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(96, (1, 1), activation=\"relu\", name=\"inception_3_/3x3_reduce\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:44: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(208, (3, 3), activation=\"relu\", name=\"inception_3_/3x3\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (1, 1), activation=\"relu\", name=\"inception_3_/5x5_reduce\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:50: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(48, (5, 5), activation=\"relu\", name=\"inception_3_/5x5\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:53: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(pool_size=(3, 3), strides=(1, 1), name=\"inception_3_/pool\", padding=\"same\")`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:56: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (1, 1), activation=\"relu\", name=\"inception_3_/pool_proj\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:38: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(160, (1, 1), activation=\"relu\", name=\"inception_4_/1x1\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:41: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(112, (1, 1), activation=\"relu\", name=\"inception_4_/3x3_reduce\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:44: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(224, (3, 3), activation=\"relu\", name=\"inception_4_/3x3\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(24, (1, 1), activation=\"relu\", name=\"inception_4_/5x5_reduce\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:50: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (5, 5), activation=\"relu\", name=\"inception_4_/5x5\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:53: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(pool_size=(3, 3), strides=(1, 1), name=\"inception_4_/pool\", padding=\"same\")`\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:56: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (1, 1), activation=\"relu\", name=\"inception_4_/pool_proj\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "inception_1_/3x3_reduce (Conv2D (None, 32, 32, 96)   384         input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "inception_1_/5x5_reduce (Conv2D (None, 32, 32, 16)   64          input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "inception_1_/pool (MaxPooling2D (None, 32, 32, 3)    0           input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "inception_1_/1x1 (Conv2D)       (None, 32, 32, 64)   256         input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "inception_1_/3x3 (Conv2D)       (None, 32, 32, 128)  110720      inception_1_/3x3_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inception_1_/5x5 (Conv2D)       (None, 32, 32, 32)   12832       inception_1_/5x5_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inception_1_/pool_proj (Conv2D) (None, 32, 32, 32)   128         inception_1_/pool[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_1_/output (Concatenat (None, 32, 32, 256)  0           inception_1_/1x1[0][0]           \n",
      "                                                                 inception_1_/3x3[0][0]           \n",
      "                                                                 inception_1_/5x5[0][0]           \n",
      "                                                                 inception_1_/pool_proj[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "inception_1_/output_norm (Batch (None, 32, 32, 256)  1024        inception_1_/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_1_2x2subsample (MaxPo (None, 16, 16, 256)  0           inception_1_/output_norm[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "inception_2_/3x3_reduce (Conv2D (None, 16, 16, 128)  32896       inception_1_2x2subsample[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "inception_2_/5x5_reduce (Conv2D (None, 16, 16, 32)   8224        inception_1_2x2subsample[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "inception_2_/pool (MaxPooling2D (None, 16, 16, 256)  0           inception_1_2x2subsample[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "inception_2_/1x1 (Conv2D)       (None, 16, 16, 128)  32896       inception_1_2x2subsample[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "inception_2_/3x3 (Conv2D)       (None, 16, 16, 192)  221376      inception_2_/3x3_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inception_2_/5x5 (Conv2D)       (None, 16, 16, 96)   76896       inception_2_/5x5_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inception_2_/pool_proj (Conv2D) (None, 16, 16, 64)   16448       inception_2_/pool[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_2_/output (Concatenat (None, 16, 16, 480)  0           inception_2_/1x1[0][0]           \n",
      "                                                                 inception_2_/3x3[0][0]           \n",
      "                                                                 inception_2_/5x5[0][0]           \n",
      "                                                                 inception_2_/pool_proj[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "inception_2_/output_norm (Batch (None, 16, 16, 480)  1920        inception_2_/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_2_2x2subsample (MaxPo (None, 8, 8, 480)    0           inception_2_/output_norm[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "inception_3_/3x3_reduce (Conv2D (None, 8, 8, 96)     46176       inception_2_2x2subsample[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "inception_3_/5x5_reduce (Conv2D (None, 8, 8, 16)     7696        inception_2_2x2subsample[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "inception_3_/pool (MaxPooling2D (None, 8, 8, 480)    0           inception_2_2x2subsample[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "inception_3_/1x1 (Conv2D)       (None, 8, 8, 192)    92352       inception_2_2x2subsample[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "inception_3_/3x3 (Conv2D)       (None, 8, 8, 208)    179920      inception_3_/3x3_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inception_3_/5x5 (Conv2D)       (None, 8, 8, 48)     19248       inception_3_/5x5_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inception_3_/pool_proj (Conv2D) (None, 8, 8, 64)     30784       inception_3_/pool[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_3_/output (Concatenat (None, 8, 8, 512)    0           inception_3_/1x1[0][0]           \n",
      "                                                                 inception_3_/3x3[0][0]           \n",
      "                                                                 inception_3_/5x5[0][0]           \n",
      "                                                                 inception_3_/pool_proj[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "inception_3_/output_norm (Batch (None, 8, 8, 512)    2048        inception_3_/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_3_2x2subsample (MaxPo (None, 4, 4, 512)    0           inception_3_/output_norm[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "inception_4_/3x3_reduce (Conv2D (None, 4, 4, 112)    57456       inception_3_2x2subsample[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "inception_4_/5x5_reduce (Conv2D (None, 4, 4, 24)     12312       inception_3_2x2subsample[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "inception_4_/pool (MaxPooling2D (None, 4, 4, 512)    0           inception_3_2x2subsample[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "inception_4_/1x1 (Conv2D)       (None, 4, 4, 160)    82080       inception_3_2x2subsample[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "inception_4_/3x3 (Conv2D)       (None, 4, 4, 224)    226016      inception_4_/3x3_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inception_4_/5x5 (Conv2D)       (None, 4, 4, 64)     38464       inception_4_/5x5_reduce[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "inception_4_/pool_proj (Conv2D) (None, 4, 4, 64)     32832       inception_4_/pool[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "inception_4_/output (Concatenat (None, 4, 4, 512)    0           inception_4_/1x1[0][0]           \n",
      "                                                                 inception_4_/3x3[0][0]           \n",
      "                                                                 inception_4_/5x5[0][0]           \n",
      "                                                                 inception_4_/pool_proj[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "inception_4_/output_norm (Batch (None, 4, 4, 512)    2048        inception_4_/output[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "inception_4_2x2subsample (MaxPo (None, 2, 2, 512)    0           inception_4_/output_norm[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 1, 1, 512)    0           inception_4_2x2subsample[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 512)          0           average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1_128 (Dense)             (None, 128)          65664       flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1_128drop (Dropout)       (None, 128)          0           dense_1_128[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1_128norm (BatchNormaliza (None, 128)          512         dense_1_128drop[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_2_64 (Dense)              (None, 64)           8256        dense_1_128norm[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_2_64drop (Dropout)        (None, 64)           0           dense_2_64[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_2_64norm (BatchNormalizat (None, 64)           256         dense_2_64drop[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           650         dense_2_64norm[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 1,420,834\n",
      "Trainable params: 1,416,930\n",
      "Non-trainable params: 3,904\n",
      "__________________________________________________________________________________________________\n",
      "已将模型储存至../models/cifar10-nrcrt7-09:05AM_April-11-2019.json\n"
     ]
    }
   ],
   "source": [
    "import datetime # 输出模型日期后缀\n",
    "\n",
    "from keras.layers import Flatten, Activation, Conv2D, MaxPool2D, AvgPool2D, Dense, Dropout, BatchNormalization, Input, MaxPooling2D, Flatten, Activation, Conv2D, AvgPool2D, Dense, Dropout, concatenate, AveragePooling2D\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.models import Sequential\n",
    "import keras.backend as K\n",
    "from keras.regularizers import l1,l2\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "from keras.models import model_from_json, Model\n",
    "\n",
    "\n",
    "\n",
    "# 自定义全连接层\n",
    "def build_dense(input_layer, neurons_nr, dense_nr, \n",
    "                dropout=False, normalization=False, regularization='l2', dropout_ratio=0.5):\n",
    "  \n",
    "    dense = Dense(neurons_nr, kernel_regularizer=regularization, \n",
    "                  name='dense_%d_%d'%(dense_nr, neurons_nr))(input_layer)\n",
    "    \n",
    "    # 视条件而定 使用dropout/normalization\n",
    "    if dropout:\n",
    "        dense = Dropout(dropout_ratio, name='dense_%d_%ddrop'%(dense_nr, neurons_nr))(dense)\n",
    "    if normalization:\n",
    "        dense = BatchNormalization(name='dense_%d_%dnorm'%(dense_nr, neurons_nr))(dense) #Batch Normalization批量标准化\n",
    "    \n",
    "    return dense\n",
    "\n",
    "  \n",
    "  \n",
    "# 构建一个Inception模型\n",
    "def build_inception_module(input_layer, features_nr, module_nr, \n",
    "                           dropout=False, normalization=False, regularization='l2', dropout_ratio=0.2): \n",
    "  \n",
    "    # feature_nr 是一个用来构建一个inception内部网络层的数组\n",
    "    # 其数据形式为: [1x1, 3x3 reduce, 3x3, 5x5 reduce, 5x5, pool proj]\n",
    "    \n",
    "    # 1*1 卷积核  \n",
    "    inception_1x1 = Conv2D(features_nr[0],1,1,border_mode='same',activation='relu',name='inception_%d_/1x1'%(module_nr),W_regularizer=l2(0.0002))(input_layer)\n",
    "    \n",
    "    # 1. 实现跨通道的交互和信息整合；2. 进行卷积核通道数的降维\n",
    "    inception_3x3_reduce = Conv2D(features_nr[1],1,1,border_mode='same',activation='relu',name='inception_%d_/3x3_reduce'%(module_nr),W_regularizer=l2(0.0002))(input_layer) \n",
    "    \n",
    "    # 3*3 卷积核\n",
    "    inception_3x3 = Conv2D(features_nr[2],3,3,border_mode='same',activation='relu',name='inception_%d_/3x3'%(module_nr),W_regularizer=l2(0.0002))(inception_3x3_reduce)\n",
    "    \n",
    "    # 1. 实现跨通道的交互和信息整合；2. 进行卷积核通道数的降维\n",
    "    inception_5x5_reduce = Conv2D(features_nr[3],1,1,border_mode='same',activation='relu',name='inception_%d_/5x5_reduce'%(module_nr),W_regularizer=l2(0.0002))(input_layer)\n",
    "    \n",
    "    # 5*5 卷积核\n",
    "    inception_5x5 = Conv2D(features_nr[4],5,5,border_mode='same',activation='relu',name='inception_%d_/5x5'%(module_nr),W_regularizer=l2(0.0002))(inception_5x5_reduce)\n",
    "    \n",
    "    # max pooling 核\n",
    "    inception_pool = MaxPooling2D(pool_size=(3,3),strides=(1,1),border_mode='same',name='inception_%d_/pool'%(module_nr))(input_layer)\n",
    "    \n",
    "    # 1. 实现跨通道的交互和信息整合；2. 进行卷积核通道数的降维\n",
    "    inception_pool_proj = Conv2D(features_nr[5],1,1,border_mode='same',activation='relu',name='inception_%d_/pool_proj'%(module_nr),W_regularizer=l2(0.0002))(inception_pool)\n",
    "    \n",
    "    # inception 输出\n",
    "    inception_output = concatenate([inception_1x1,inception_3x3,inception_5x5,inception_pool_proj],axis=3,name='inception_%d_/output'%(module_nr))\n",
    "\n",
    "    # 视条件而定 使用dropout/normalization\n",
    "    if dropout:\n",
    "        inception_output = Dropout(dropout_ratio, name='inception_%d_/output_drop'%(module_nr))(inception_output)\n",
    "    if normalization:\n",
    "        inception_output = BatchNormalization(name='inception_%d_/output_norm'%(module_nr))(inception_output)\n",
    "    \n",
    "    # maxpooling层最终输出（2*2）\n",
    "    pooled = MaxPooling2D((2,2), padding='same', name='inception_%d_2x2subsample'%(module_nr))(inception_output)\n",
    "    \n",
    "    return pooled\n",
    "\n",
    "#模型名称\n",
    "i='cifar10-nrcrt7-'+datetime.datetime.now().strftime(\"%I:%M%p_%B-%d-%Y\")\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "#在Google云盘创建储存模型与日志的文件夹（工作目录下创建）\n",
    "!mkdir -p models\n",
    "!mkdir -p logs\n",
    "\n",
    "a = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')#如果验证集loss值连续10个周期不下降，程序自动停止（早停法）\n",
    "b = ModelCheckpoint(monitor='val_loss', filepath='./models/'+str(i)+'.hdf5', verbose=1, save_best_only=True)#每个训练周期后，验证集loss值如果下降，则储存改模型（最终只储存最好的模型）\n",
    "c = TensorBoard(log_dir='./logs/'+str(i),\n",
    "                write_grads=True,\n",
    "                write_graph=True,\n",
    "                write_images=True,\n",
    "                batch_size=256)#保存日志文件至Google云盘中\n",
    "\n",
    "#回调函数：当评价指标（验证集loss值）不在提升时，减少学习率 （loss值连续patience次没有变化时，学习率缩小为factor倍）\n",
    "d = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=4, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "\n",
    "callbacks=[a,b,c,d]\n",
    "\n",
    "#------------模型定义-------------------\n",
    "\n",
    "use_norm = True #使用BN\n",
    "lrate = 0.001 #学习率\n",
    "\n",
    "input_img = Input(shape = (32, 32, 3), name='input') #数据输入\n",
    "\n",
    "inception_1 = build_inception_module(input_img, [64,96,128,16,32,32], 1, False, use_norm) #inception_1\n",
    "\n",
    "inception_2 = build_inception_module(inception_1, [128,128,192,32,96,64], 2, False, use_norm)#inception_2\n",
    "\n",
    "inception_3 = build_inception_module(inception_2, [192,96,208,16,48,64], 3, False, use_norm)#inception_3\n",
    "\n",
    "inception_4 = build_inception_module(inception_3, [160, 112, 224, 24, 64, 64], 4, False, use_norm)#inception_4\n",
    "\n",
    "flat_pool = AveragePooling2D(pool_size=(2, 2), padding='valid')(inception_4) #平均池化\n",
    "\n",
    "flat = Flatten()(flat_pool)\n",
    "\n",
    "dense_5 = build_dense(flat, 128, 1, True, use_norm) # 全连接层\n",
    "\n",
    "dense_6 = build_dense(dense_5, 64, 2, True, use_norm) # 全连接层\n",
    "\n",
    "out = Dense(10, activation='softmax')(dense_6) # 最后一层使用softmax激活函数\n",
    "\n",
    "model = Model(inputs = input_img, outputs = out)# 输出\n",
    "\n",
    "#-----------------------------------------------\n",
    "\n",
    "model.compile(loss='binary_crossentropy', #二分类的损失函数\n",
    "              optimizer=Adam(lrate),\n",
    "              metrics=['accuracy']) #设置损失函数和优化器\n",
    "\n",
    "model.summary()\n",
    "\n",
    "#将模型转换为json文件\n",
    "model_json = model.to_json()\n",
    "with open(\"./models/\"+str(i)+\".json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "print(\"已将模型储存至\" + \"../models/\"+str(i)+\".json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QDChX97kFYUu"
   },
   "source": [
    "## 模型可视化\n",
    "本模型包含4个线性堆叠的Inception层，3层全连接层，\n",
    "\n",
    "该模型在最后一个 inception 模块处使用了全局平均池化，最后一个全连接层输出时使用sofmax激活函数。\n",
    "\n",
    "Dropout可有效防止过拟合的发生,但从深度学习的发展趋势看，Batch Normalizaton（BN）正在逐步取代Dropout技术，特别是在卷积层。\n",
    "\n",
    "BN在准确率和损失率上表现要优于Dropout，故本文在Inception层采用了BN，在全连接层继续采用了Dropout。\n",
    "\n",
    "\n",
    "\n",
    "![alt text](https://github.com/MirstT/Colab_Cifar10_Image-recognition/blob/master/res/%E6%A8%A1%E5%9E%8B%E6%A1%86%E6%9E%B6.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sMO2DT7VljF6"
   },
   "source": [
    "## 运行内存\n",
    "模型运行时内存统计（参数）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "m2TDH7vMiCte",
    "outputId": "421915e9-1e26-4ee6-eda3-df6cde25190f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "内存使用 (GB): 1.47\n"
     ]
    }
   ],
   "source": [
    "def get_model_memory_usage(batch_size, model):\n",
    "    import numpy as np\n",
    "    from keras import backend as K\n",
    "\n",
    "    shapes_mem_count = 0\n",
    "    for l in model.layers:\n",
    "        single_layer_mem = 1\n",
    "        for s in l.output_shape:\n",
    "            if s is None:\n",
    "                continue\n",
    "            single_layer_mem *= s\n",
    "        shapes_mem_count += single_layer_mem\n",
    "\n",
    "    trainable_count = np.sum([K.count_params(p) for p in set(model.trainable_weights)])\n",
    "    non_trainable_count = np.sum([K.count_params(p) for p in set(model.non_trainable_weights)])\n",
    "\n",
    "    total_memory = 4.0*batch_size*(shapes_mem_count + trainable_count + non_trainable_count)\n",
    "    gbytes = np.round(total_memory / (1024.0 ** 3), 3)\n",
    "    return gbytes\n",
    "  \n",
    "print(\"内存使用 (GB):\", get_model_memory_usage(128,model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gSLsfX-EeasE"
   },
   "source": [
    "## 模型训练\n",
    "使用GPU加速训练过程，并将训练所得模型保存至Google云盘。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3023
    },
    "colab_type": "code",
    "id": "6XB2xRoYiMSK",
    "outputId": "7b106e48-0861-4874-a87a-e19b8d471c9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "40000/40000 [==============================] - 65s 2ms/step - loss: 1.3503 - acc: 0.9062 - val_loss: 0.6279 - val_acc: 0.9000\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.62791, saving model to ./models/cifar10-nrcrt7-09:05AM_April-11-2019.hdf5\n",
      "Epoch 2/100\n",
      "40000/40000 [==============================] - 62s 2ms/step - loss: 0.4296 - acc: 0.9288 - val_loss: 0.4376 - val_acc: 0.9043\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.62791 to 0.43763, saving model to ./models/cifar10-nrcrt7-09:05AM_April-11-2019.hdf5\n",
      "Epoch 3/100\n",
      "40000/40000 [==============================] - 62s 2ms/step - loss: 0.3087 - acc: 0.9401 - val_loss: 0.3233 - val_acc: 0.9237\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.43763 to 0.32332, saving model to ./models/cifar10-nrcrt7-09:05AM_April-11-2019.hdf5\n",
      "Epoch 4/100\n",
      "40000/40000 [==============================] - 62s 2ms/step - loss: 0.2534 - acc: 0.9474 - val_loss: 0.2844 - val_acc: 0.9299\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.32332 to 0.28435, saving model to ./models/cifar10-nrcrt7-09:05AM_April-11-2019.hdf5\n",
      "Epoch 5/100\n",
      "40000/40000 [==============================] - 62s 2ms/step - loss: 0.2240 - acc: 0.9522 - val_loss: 0.2754 - val_acc: 0.9271\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.28435 to 0.27542, saving model to ./models/cifar10-nrcrt7-09:05AM_April-11-2019.hdf5\n",
      "Epoch 6/100\n",
      "40000/40000 [==============================] - 62s 2ms/step - loss: 0.2058 - acc: 0.9565 - val_loss: 0.2405 - val_acc: 0.9408\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.27542 to 0.24050, saving model to ./models/cifar10-nrcrt7-09:05AM_April-11-2019.hdf5\n",
      "Epoch 7/100\n",
      "40000/40000 [==============================] - 62s 2ms/step - loss: 0.1932 - acc: 0.9596 - val_loss: 0.2350 - val_acc: 0.9434\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.24050 to 0.23501, saving model to ./models/cifar10-nrcrt7-09:05AM_April-11-2019.hdf5\n",
      "Epoch 8/100\n",
      "40000/40000 [==============================] - 62s 2ms/step - loss: 0.1831 - acc: 0.9624 - val_loss: 0.2936 - val_acc: 0.9255\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.23501\n",
      "Epoch 9/100\n",
      "40000/40000 [==============================] - 62s 2ms/step - loss: 0.1786 - acc: 0.9639 - val_loss: 0.2312 - val_acc: 0.9432\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.23501 to 0.23119, saving model to ./models/cifar10-nrcrt7-09:05AM_April-11-2019.hdf5\n",
      "Epoch 10/100\n",
      "40000/40000 [==============================] - 62s 2ms/step - loss: 0.1748 - acc: 0.9654 - val_loss: 0.2121 - val_acc: 0.9516\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.23119 to 0.21211, saving model to ./models/cifar10-nrcrt7-09:05AM_April-11-2019.hdf5\n",
      "Epoch 11/100\n",
      "40000/40000 [==============================] - 62s 2ms/step - loss: 0.1671 - acc: 0.9680 - val_loss: 0.2327 - val_acc: 0.9449\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.21211\n",
      "Epoch 12/100\n",
      "40000/40000 [==============================] - 62s 2ms/step - loss: 0.1656 - acc: 0.9687 - val_loss: 0.2290 - val_acc: 0.9446\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.21211\n",
      "Epoch 13/100\n",
      "40000/40000 [==============================] - 62s 2ms/step - loss: 0.1586 - acc: 0.9706 - val_loss: 0.2428 - val_acc: 0.9416\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.21211\n",
      "Epoch 14/100\n",
      "40000/40000 [==============================] - 62s 2ms/step - loss: 0.1579 - acc: 0.9712 - val_loss: 0.1954 - val_acc: 0.9580\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.21211 to 0.19537, saving model to ./models/cifar10-nrcrt7-09:05AM_April-11-2019.hdf5\n",
      "Epoch 15/100\n",
      "40000/40000 [==============================] - 62s 2ms/step - loss: 0.1532 - acc: 0.9727 - val_loss: 0.2572 - val_acc: 0.9365\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.19537\n",
      "Epoch 16/100\n",
      "40000/40000 [==============================] - 62s 2ms/step - loss: 0.1489 - acc: 0.9741 - val_loss: 0.2141 - val_acc: 0.9535\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.19537\n",
      "Epoch 17/100\n",
      "40000/40000 [==============================] - 62s 2ms/step - loss: 0.1472 - acc: 0.9748 - val_loss: 0.2369 - val_acc: 0.9481\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.19537\n",
      "Epoch 18/100\n",
      "40000/40000 [==============================] - 62s 2ms/step - loss: 0.1465 - acc: 0.9752 - val_loss: 0.2029 - val_acc: 0.9551\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.19537\n",
      "Epoch 19/100\n",
      "40000/40000 [==============================] - 62s 2ms/step - loss: 0.1106 - acc: 0.9865 - val_loss: 0.1498 - val_acc: 0.9688\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.19537 to 0.14979, saving model to ./models/cifar10-nrcrt7-09:05AM_April-11-2019.hdf5\n",
      "Epoch 20/100\n",
      "40000/40000 [==============================] - 62s 2ms/step - loss: 0.0884 - acc: 0.9919 - val_loss: 0.1437 - val_acc: 0.9689\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.14979 to 0.14373, saving model to ./models/cifar10-nrcrt7-09:05AM_April-11-2019.hdf5\n",
      "Epoch 21/100\n",
      "40000/40000 [==============================] - 62s 2ms/step - loss: 0.0770 - acc: 0.9941 - val_loss: 0.1569 - val_acc: 0.9652\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.14373\n",
      "Epoch 22/100\n",
      "40000/40000 [==============================] - 62s 2ms/step - loss: 0.0709 - acc: 0.9953 - val_loss: 0.1688 - val_acc: 0.9631\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.14373\n",
      "Epoch 23/100\n",
      "40000/40000 [==============================] - 63s 2ms/step - loss: 0.0663 - acc: 0.9958 - val_loss: 0.1616 - val_acc: 0.9650\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.14373\n",
      "Epoch 24/100\n",
      "40000/40000 [==============================] - 62s 2ms/step - loss: 0.0644 - acc: 0.9957 - val_loss: 0.1823 - val_acc: 0.9622\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.14373\n",
      "Epoch 25/100\n",
      "40000/40000 [==============================] - 62s 2ms/step - loss: 0.0561 - acc: 0.9984 - val_loss: 0.1395 - val_acc: 0.9708\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.14373 to 0.13948, saving model to ./models/cifar10-nrcrt7-09:05AM_April-11-2019.hdf5\n",
      "Epoch 26/100\n",
      "40000/40000 [==============================] - 62s 2ms/step - loss: 0.0509 - acc: 0.9994 - val_loss: 0.1379 - val_acc: 0.9705\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.13948 to 0.13788, saving model to ./models/cifar10-nrcrt7-09:05AM_April-11-2019.hdf5\n",
      "Epoch 27/100\n",
      "40000/40000 [==============================] - 62s 2ms/step - loss: 0.0479 - acc: 0.9997 - val_loss: 0.1360 - val_acc: 0.9704\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.13788 to 0.13597, saving model to ./models/cifar10-nrcrt7-09:05AM_April-11-2019.hdf5\n",
      "Epoch 28/100\n",
      "40000/40000 [==============================] - 63s 2ms/step - loss: 0.0459 - acc: 0.9998 - val_loss: 0.1359 - val_acc: 0.9706\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.13597 to 0.13591, saving model to ./models/cifar10-nrcrt7-09:05AM_April-11-2019.hdf5\n",
      "Epoch 29/100\n",
      "40000/40000 [==============================] - 62s 2ms/step - loss: 0.0439 - acc: 0.9998 - val_loss: 0.1357 - val_acc: 0.9697\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.13591 to 0.13572, saving model to ./models/cifar10-nrcrt7-09:05AM_April-11-2019.hdf5\n",
      "Epoch 30/100\n",
      "40000/40000 [==============================] - 62s 2ms/step - loss: 0.0421 - acc: 0.9999 - val_loss: 0.1342 - val_acc: 0.9703\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.13572 to 0.13419, saving model to ./models/cifar10-nrcrt7-09:05AM_April-11-2019.hdf5\n",
      "Epoch 31/100\n",
      "40000/40000 [==============================] - 62s 2ms/step - loss: 0.0410 - acc: 0.9999 - val_loss: 0.1365 - val_acc: 0.9701\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.13419\n",
      "Epoch 32/100\n",
      "40000/40000 [==============================] - 62s 2ms/step - loss: 0.0393 - acc: 0.9999 - val_loss: 0.1365 - val_acc: 0.9699\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.13419\n",
      "Epoch 33/100\n",
      "40000/40000 [==============================] - 62s 2ms/step - loss: 0.0383 - acc: 0.9999 - val_loss: 0.1403 - val_acc: 0.9701\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.13419\n",
      "Epoch 34/100\n",
      "40000/40000 [==============================] - 62s 2ms/step - loss: 0.0378 - acc: 0.9998 - val_loss: 0.1434 - val_acc: 0.9692\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.13419\n",
      "Epoch 35/100\n",
      "40000/40000 [==============================] - 62s 2ms/step - loss: 0.0361 - acc: 1.0000 - val_loss: 0.1396 - val_acc: 0.9700\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.13419\n",
      "Epoch 36/100\n",
      "40000/40000 [==============================] - 63s 2ms/step - loss: 0.0354 - acc: 1.0000 - val_loss: 0.1394 - val_acc: 0.9700\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.13419\n",
      "Epoch 37/100\n",
      "40000/40000 [==============================] - 62s 2ms/step - loss: 0.0347 - acc: 1.0000 - val_loss: 0.1394 - val_acc: 0.9697\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.13419\n",
      "Epoch 38/100\n",
      "40000/40000 [==============================] - 62s 2ms/step - loss: 0.0342 - acc: 1.0000 - val_loss: 0.1377 - val_acc: 0.9698\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.13419\n",
      "Epoch 39/100\n",
      "40000/40000 [==============================] - 62s 2ms/step - loss: 0.0338 - acc: 1.0000 - val_loss: 0.1404 - val_acc: 0.9699\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.13419\n",
      "Epoch 40/100\n",
      "40000/40000 [==============================] - 62s 2ms/step - loss: 0.0336 - acc: 1.0000 - val_loss: 0.1399 - val_acc: 0.9700\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.13419\n",
      "Epoch 00040: early stopping\n",
      "10000/10000 [==============================] - 7s 729us/step\n",
      "准确率（测试集）:  96.89600015640258 %\n",
      "cp: 'models' and './models' are the same file\n",
      "cp: 'logs' and './logs' are the same file\n",
      "已将模型与日志拷贝至Google云盘\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "  model.fit(x_train, y_train_cat, batch_size=256, epochs=100, validation_split=0.2,verbose=1,callbacks=callbacks)  # 开始训练 100个周期\n",
    "\n",
    "  \n",
    "  \n",
    "result = model.evaluate(x_test, y_test_cat)\n",
    "\n",
    "print(\"准确率（测试集）: \",result[1]*100,\"%\")\n",
    "\n",
    "#将模型与日志拷贝至Google云盘\n",
    "!cp -R models ./\n",
    "!cp -R logs ./\n",
    "\n",
    "print(\"已将模型与日志拷贝至Google云盘\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZLMzGIZMenWt"
   },
   "source": [
    "## 结果分析\n",
    "\n",
    "随着batch的增大，可以明显的看到模型每个周期的训练时间有所减少，\n",
    "\n",
    "增大batch值确实可以提升每个周期的训练速度，但是也容易在训练集上产生过拟合，\n",
    "\n",
    "当batch增大至512时，最终在训练集上的准确度甚至可以达到了惊人的100.00%。\n",
    "\n",
    "本文模型采用早停法与ReduceLROnPlateau回调函数，\n",
    "\n",
    "当模型不再收敛时也会自动的缩小学习率，以达到最佳优化效果（从图中曲线变化也看出了学习率调整对模型收敛的帮助）\n",
    "\n",
    "基本上所有的训练都在30个左右周期时自动停止，\n",
    "\n",
    "且随着batch的增大，模型的收敛过程有所延后，\n",
    "\n",
    "batch值增大虽然能够为每个周期训练时间上的减少，\n",
    "\n",
    "但是收敛过程延后，总体周期数量增加，模型的总的训练时间反而随着batch的增大而增长了\n",
    "\n",
    "\n",
    "本文采用的是最原始的inception-v1模型，已经将数据集拟合的相当不错，\n",
    "\n",
    "后续学习中将会挑选复杂度更大的cifar100数据集，使用最新的inception-v4，来加深对相关理论知识的理解。\n",
    "\n",
    "### 标签\n",
    "![alt text](https://github.com/MirstT/Colab_Cifar10_Image-recognition/blob/master/res/%E6%A0%87%E7%AD%BE.jpg?raw=true)\n",
    "\n",
    "### 训练集\n",
    "![alt text](https://github.com/MirstT/Colab_Cifar10_Image-recognition/blob/master/res/%E8%AE%AD%E7%BB%83%E9%9B%86acc.png?raw=true)\n",
    "![alt text](https://github.com/MirstT/Colab_Cifar10_Image-recognition/blob/master/res/%E8%AE%AD%E7%BB%83%E9%9B%86loss.png?raw=true)\n",
    "\n",
    "### 验证集\n",
    "![alt text](https://github.com/MirstT/Colab_Cifar10_Image-recognition/blob/master/res/%E9%AA%8C%E8%AF%81%E9%9B%86acc.png?raw=true)\n",
    "![alt text](https://github.com/MirstT/Colab_Cifar10_Image-recognition/blob/master/res/%E9%AA%8C%E8%AF%81%E9%9B%86loss.png?raw=true)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ItkPGXmeedDB"
   },
   "source": [
    "## 模型测试\n",
    "cifar10-nrcrt7-09:19AM_April-09-2019.hdf5\n",
    "\n",
    "导入模型，在测试集上进行测试，输出loss值和准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "b_YyW_4MkCIE",
    "outputId": "86b7dec3-e8c9-4677-c2bb-ff3a854697ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 7s 731us/step\n",
      "[0.1417611664056778, 0.9691200012207031]\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('./models/cifar10-nrcrt7-09:05AM_April-11-2019.hdf5')\n",
    "\n",
    "result = model.evaluate(x_test, y_test_cat)      \n",
    "\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Cifar_10_图像识别.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
